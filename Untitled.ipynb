{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install trax==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/siddarthathentu/Desktop/NLP/LSTM-NER\n"
     ]
    }
   ],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "from utils import get_vocab,get_params\n",
    "\n",
    "#set random seeds to make this notebook easier to replicate\n",
    "trax.supervised.trainer_lib.init_random_number_generators(33)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploring the data\n",
    "\n",
    "We will be using a dataset from Kaggle. The original data consists of four columns, the sentence number, the word, the part of speech of the word, and the tags. A few tags you might expect to see are:\n",
    "\n",
    "    geo: geographical entity\n",
    "    org: organization\n",
    "    per: person\n",
    "    gpe: geopolitical entity\n",
    "    tim: time indicator\n",
    "    art: artifact\n",
    "    eve: event\n",
    "    nat: natural phenomenon\n",
    "    O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE :  Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL : O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "      Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "#display original kaggle data\n",
    "data = pd.read_csv(\"ner_dataset.csv\",encoding=\"ISO-8859-1\")\n",
    "train_sents = open('data/small/train/sentences.txt',\"r\").readline()\n",
    "train_labels = open('data/small/train/labels.txt',\"r\").readline()\n",
    "print(\"SENTENCE : \",train_sents)\n",
    "print(\"SENTENCE LABEL :\",train_labels)\n",
    "print(\"ORIGINAL DATA:\\n \",data.head(5))\n",
    "del(data,train_sents,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Importing the data\n",
    "In this part, we will import the preprocessed data and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,tag_map = get_vocab(\"data/large/words.txt\",\"data/large/tags.txt\")\n",
    "t_sentences,t_labels,t_size = get_params(vocab,tag_map,'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences,v_labels,v_size = get_params(vocab,tag_map,'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences,test_labels,test_size = get_params(vocab,tag_map,'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a <PAD> token.\n",
    "\n",
    "When training an LSTM using batches, all our input sentences must be the same size. To accomplish this, we set the length of your sentences to a certain number and add the generic <PAD> token to fill all the empty spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[the] :  9\n",
      "padded token :   35180\n"
     ]
    }
   ],
   "source": [
    "#vocab translates from a word to a unique number\n",
    "print(\"vocab[the] : \",vocab[\"the\"])\n",
    "#pad token\n",
    "print(\"padded token :  \",vocab[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag_map corresponds to one of the possible tags a word can have. Run the cell below to see the possible classes we will be predicting. The prepositions in the tags mean:\n",
    "\n",
    "    I: Token is inside an entity.\n",
    "    B: Token begins an entity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If we had the sentence\n",
    "\n",
    "\"Sharon flew to Miami on Friday\"\n",
    "\n",
    "the outputs would look like:\n",
    "\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "\n",
    "our tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if we added Sharon's last name to the sentence:\n",
    "\n",
    "\"Sharon Floyd flew to Miami on Friday\"\n",
    "\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "\n",
    "then our tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then number of outputs in tag_map :   17\n",
      "Number of vocabulary words ;  35181\n",
      "The training size is  33570\n",
      "The validation size is  7194\n",
      "An example of first sentence is :\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
      "\n",
      "An example of its corresponding label is  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Exploring information about the data\n",
    "print(\"Then number of outputs in tag_map :  \",len(tag_map))\n",
    "#Number of vocavulary tokens including PAD\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of vocabulary words ; \",vocab_size)\n",
    "print(\"The training size is \",t_size)\n",
    "print(\"The validation size is \",v_size)\n",
    "print(\"An example of first sentence is :\\n\")\n",
    "print(t_sentences[0])\n",
    "print(\"\\nAn example of its corresponding label is \",t_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible classes, as shown in the tag map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Data generator¶\n",
    "\n",
    "In python, a generator is a function that behaves like an iterator. It will return the next item. \n",
    "\n",
    "In many AI applications it is very useful to have a data generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size,x,y,pad,shuffle=False,verbose=False):\n",
    "    \n",
    "    num_lines = len(x)\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        buffer_x = [0]*batch_size\n",
    "        buffer_y = [0]*batch_size\n",
    "        \n",
    "        maxLength = 0\n",
    "        for i in range(batch_size):\n",
    "            if index>=num_lines:\n",
    "                index = 0\n",
    "                #re-shuffle\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(lines_index)\n",
    "            \n",
    "            buffer_x[i] = x[lines_index[index]]\n",
    "            buffer_y[i] = y[lines_index[index]]\n",
    "            \n",
    "            maxLength = max(maxLength,len(buffer_x[i]))\n",
    "            \n",
    "            index+=1\n",
    "            \n",
    "        X = np.full((batch_size,maxLength),pad)\n",
    "        Y = np.full((batch_size,maxLength),pad)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            x_i = buffer_x[i]\n",
    "            y_i = buffer_y[i]\n",
    "            \n",
    "            for j in range(len(x_i)):\n",
    "                X[i,j] = x_i[j]\n",
    "                Y[i,j] = y_i[j]\n",
    "        \n",
    "        if verbose:print(\"index = \",index)\n",
    "        yield((X,Y))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index =  5\n",
      "index =  2\n",
      "(5, 30) (5, 30) (5, 30) (5, 30)\n",
      "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14     9    15     1    16    17    18    19    20    21\n",
      " 35180 35180 35180 35180 35180 35180] \n",
      " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
      "     1     0     0     0     0     0     2     0     0     0     0     0\n",
      " 35180 35180 35180 35180 35180 35180]\n"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "mini_sentences = t_sentences[0:8]\n",
    "mini_labels = t_labels[0:8]\n",
    "data_gen = data_generator(batch_size,mini_sentences,mini_labels,vocab[\"<PAD>\"],shuffle=False,verbose=True)\n",
    "X1,Y1 = next(data_gen)\n",
    "X2,Y2 = next(data_gen)\n",
    "print(Y1.shape,X1.shape,Y2.shape,X2.shape)\n",
    "print(X1[0][:],\"\\n\",Y1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Building the model\n",
    "\n",
    "We will now implement the model. We will be using Google's TensorFlow. our model will be able to distinguish the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows: \n",
    "\n",
    "<img src = 'ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "Concretely: \n",
    "\n",
    "* Use the input tensors you built in our data generator\n",
    "* Feed it into an Embedding layer, to produce more semantic entries\n",
    "* Feed it into an LSTM layer\n",
    "* Run the output through a linear layer\n",
    "* Run the result through a log softmax layer to get the predicted class for each word.\n",
    "\n",
    "We won't implement the LSTM unit drawn above. However, we will build the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(vocab_size=35181,d_model=50,tags=tag_map):\n",
    "    \n",
    "    model = tl.Serial(\n",
    "            tl.Embedding(vocab_size,d_model),\n",
    "            tl.LSTM(n_units=d_model),\n",
    "            tl.Dense(n_units=len(tags)),\n",
    "            tl.LogSoftmax()\n",
    "            )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Embedding_35181_50\n",
      "  LSTM_50\n",
      "  Dense_17\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NER()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Train the Model \n",
    "\n",
    "This section will train our model.\n",
    "\n",
    "Before we start, we need to create the data generators for training and validation data. It is important that we mask padding in the loss weights of your data, which can be done using the `id_to_mask` argument of `trax.supervised.inputs.add_loss_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "rnd.seed(33)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "train_generator = trax.supervised.inputs.add_loss_weights(\n",
    "                  data_generator(batch_size,t_sentences,t_labels,vocab[\"<PAD>\"],True),\n",
    "                  id_to_mask=vocab[\"<PAD>\"])\n",
    "\n",
    "eval_generator = trax.supervised.inputs.add_loss_weights(\n",
    "                 data_generator(batch_size,v_sentences,v_labels,vocab[\"<PAD>\"],True),\n",
    "                 id_to_mask=vocab[\"<PAD>\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(NER,train_generator,eval_generator,train_steps=1,output_dir=\"/home/siddarthathentu/Desktop/NLP/LSTM-NER/model\"):\n",
    "    \n",
    "    train_task = training.TrainTask(\n",
    "                 train_generator,\n",
    "                 loss_layer = tl.CrossEntropyLoss(),\n",
    "                 optimizer = trax.optimizers.Adam(0.01))\n",
    "    \n",
    "    eval_task = training.EvalTask(\n",
    "                 labeled_data = eval_generator,\n",
    "                 metrics = [tl.CrossEntropyLoss(),tl.Accuracy()],\n",
    "                 n_eval_batches = 10)\n",
    "    \n",
    "    \n",
    "    training_loop = training.Loop(\n",
    "                 NER,\n",
    "                 train_task,\n",
    "                 eval_task = eval_task,\n",
    "                 output_dir = output_dir)\n",
    "    \n",
    "    training_loop.run(n_steps=train_steps)\n",
    "    \n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step      1: train CrossEntropyLoss |  2.89228582\n",
      "Step      1: eval  CrossEntropyLoss |  1.91789852\n",
      "Step      1: eval          Accuracy |  0.66664734\n",
      "Step    100: train CrossEntropyLoss |  0.60055101\n",
      "Step    100: eval  CrossEntropyLoss |  0.36314742\n",
      "Step    100: eval          Accuracy |  0.91259873\n",
      "Step    200: train CrossEntropyLoss |  0.30090800\n",
      "Step    200: eval  CrossEntropyLoss |  0.24988867\n",
      "Step    200: eval          Accuracy |  0.93519678\n",
      "Step    300: train CrossEntropyLoss |  0.23232746\n",
      "Step    300: eval  CrossEntropyLoss |  0.21319010\n",
      "Step    300: eval          Accuracy |  0.94073426\n",
      "Step    400: train CrossEntropyLoss |  0.19600554\n",
      "Step    400: eval  CrossEntropyLoss |  0.19388027\n",
      "Step    400: eval          Accuracy |  0.94635340\n",
      "Step    500: train CrossEntropyLoss |  0.18234240\n",
      "Step    500: eval  CrossEntropyLoss |  0.17009229\n",
      "Step    500: eval          Accuracy |  0.95031501\n",
      "Step    600: train CrossEntropyLoss |  0.15213504\n",
      "Step    600: eval  CrossEntropyLoss |  0.16813952\n",
      "Step    600: eval          Accuracy |  0.95214773\n",
      "Step    700: train CrossEntropyLoss |  0.14510684\n"
     ]
    }
   ],
   "source": [
    "train_steps=1000\n",
    "!rm -rf \"model/model.pkl.gz\"\n",
    "\n",
    "training_loop = train_model(NER(),train_generator,eval_generator,train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-14278d208389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_steps' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pretrained model\n",
    "model = NER()\n",
    "model.init(trax.shapes.ShapeDtype((1,1),dtype=np.int32))\n",
    "\n",
    "model.init_from_file(\"model/pre-model.pkl.gz\",weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Compute Accuracy\n",
    "\n",
    "We will now evaluate in the test set. Previously, we have seen the accuracy on the training set and the validation (noted as eval) set. We will now evaluate on your test set. To get a good evaluation, we will need to create a mask to avoid counting the padding tokens when computing the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a comparision on a matrix \n",
    "a = np.array([1, 2, 3, 4])\n",
    "a == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shapes (7194, 70) (7194, 70)\n"
     ]
    }
   ],
   "source": [
    "#generate test data\n",
    "x, y = next(data_generator(len(test_sentences), test_sentences, test_labels, vocab['<PAD>']))\n",
    "print(\"input shapes\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.interpreters.xla.DeviceArray'>\n",
      "Shape of tmp_pred :  (7194, 70, 17)\n"
     ]
    }
   ],
   "source": [
    "#sample preiction\n",
    "tmp_pred = model(x)\n",
    "print(type(tmp_pred))\n",
    "print(\"Shape of tmp_pred : \",tmp_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.48355865e+00 -3.36047173e-01 -3.35761213e+00 ... -6.91771650e+00\n",
      "  -8.58630657e+00 -7.36349249e+00]\n",
      " [-1.18455887e-02 -8.34251595e+00 -9.30842590e+00 ... -1.01982880e+01\n",
      "  -9.44873428e+00 -9.18822384e+00]\n",
      " [-2.06794739e-02 -7.45346165e+00 -1.26838894e+01 ... -1.15326271e+01\n",
      "  -1.26500635e+01 -1.19806232e+01]\n",
      " ...\n",
      " [-1.46134377e-01 -6.71558952e+00 -9.38210487e+00 ... -9.09346294e+00\n",
      "  -8.63788605e+00 -8.78505135e+00]\n",
      " [-1.46547794e-01 -6.71508074e+00 -9.38067627e+00 ... -9.09435844e+00\n",
      "  -8.63772011e+00 -8.78440857e+00]\n",
      " [-1.46930695e-01 -6.71461391e+00 -9.37936115e+00 ... -9.09518337e+00\n",
      "  -8.63756752e+00 -8.78381729e+00]]\n"
     ]
    }
   ],
   "source": [
    "#print(x[0])\n",
    "print(tmp_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model's prediction has 3 axes: \n",
    "- the number of examples\n",
    "- the number of words in each example (padded to be as long as the longest sentence in the batch)\n",
    "- the number of possible targets (the 17 named entity tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(pred,labels,pad):\n",
    "    \n",
    "    outputs = np.argmax(pred,axis=-1)\n",
    "    mask = 1 - (labels==pad)\n",
    "    numerator = np.sum(labels==outputs)\n",
    "    denominator = np.sum(mask)\n",
    "    print(numerator,denominator)\n",
    "    acc = numerator/denominator\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149106 156234\n",
      "Accuracy of the model =  0.9543761281155191\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_prediction(model(x),y,vocab[\"<PAD>\"])\n",
    "print(\"Accuracy of the model = \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Testing with our own sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence,model,vocab,tag_map):\n",
    "    \n",
    "    s = [vocab[token] if token in vocab else vocab[\"UNK\"] for token in sentence.split(' ')]\n",
    "    \n",
    "    batch_data = np.ones((1,len(s)))\n",
    "    \n",
    "    batch_data[0][:] = s\n",
    "    \n",
    "    sentence = np.array(batch_data).astype(int)\n",
    "    output = model(sentence)\n",
    "    outputs = np.argmax(output,axis=-1)\n",
    "    labels = list(tag_map.keys())\n",
    "    \n",
    "    pred = []\n",
    "    \n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i]\n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter    B-per\n",
      "Navarro,    I-per\n",
      "White    B-org\n",
      "House    I-org\n",
      "Sunday    B-tim\n",
      "morning    I-tim\n",
      "White    B-org\n",
      "House    I-org\n",
      "coronavirus    B-tim\n",
      "fall,    B-tim\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
    "predictions = predict(sentence,model,vocab,tag_map)\n",
    "\n",
    "for x,y in zip(sentence.split(' '),predictions):\n",
    "    if y!='O':\n",
    "        print(x, \"  \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
